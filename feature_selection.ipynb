{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####import package ###\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split,cross_val_score    ###\n",
    "from sklearn.metrics import confusion_matrix,matthews_corrcoef,make_scorer,roc_curve, auc,roc_auc_score,precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.utils import shuffle  \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_excel('C:/DM/Diabetes/data.xlsx')\n",
    "\n",
    "\n",
    "# categorical features:\n",
    "X_CAT= ['Retinopathy', 'Fatty Liver',  'Hypertension2', 'Lipid Drug', 'Diabetes Drug',   \n",
    "        'Insulin Use', 'FH of Diabetes', 'FH of Hypertension', 'FH of CAD',  'Sex','Neuropathy', \n",
    "        'Diabetic Foot', 'CVD', 'Smoke'\n",
    "         ]\n",
    " \n",
    "    \n",
    "# continous features:   \n",
    "X_CON = ['Diabetes Duration', 'BMI', 'Waist/Hip', 'SBP', 'DBP', 'Na', 'K',\n",
    "         'Vitamin D', 'FBS', '2HPP', 'HbA1c3', 'Serum Insulin', 'Cholesterol', 'HDL',\n",
    "         'LDL', 'Triglyceride', 'Uric Acid', 'AST', 'ALT', 'ALKP', 'Microalbumin', 'eGFR', 'Platelet count' \n",
    "        ]\n",
    "         \n",
    "\n",
    "# split into input and output elements\n",
    "c = len(data.columns)-1\n",
    "X = data.iloc[:,1:c]\n",
    "\n",
    "y= pd.DataFrame(data, index= data.index, columns= ['DN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                            Remove Missing values                            #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "#Categorical variables -> Replace Nan with Mode\n",
    "X[X_CAT]= X[X_CAT].fillna(X[X_CAT].mode().iloc[0])\n",
    "\n",
    "\n",
    "#Non-Categorical variables -> Replace Nan with Mean\n",
    "X[X_CON]= X[X_CON].fillna(X[X_CON].mean())\n",
    "\n",
    "# print total missing\n",
    "print('X Dimension:', X.shape)\n",
    "print('X Null Count:', X.isnull().sum())\n",
    "\n",
    "\n",
    "X[X_CON].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                          Create train and test set                         #\n",
    "###############################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,\n",
    "                                                    random_state = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                         Normalize Continous features                      #\n",
    "###############################################################################\n",
    "from sklearn.preprocessing import StandardScaler#, Imputer\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[X_CON] = scaler.fit_transform(X_train[X_CON])\n",
    "X_test[X_CON] = scaler.fit_transform(X_test[X_CON])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                                Classifiers                                #\n",
    "###############################################################################\n",
    "# Create list of tuples with classifier label and classifier obj\n",
    "\n",
    "classifiers = {}\n",
    "classifiers.update({\"XGBOOST\": XGBClassifier()})\n",
    "classifiers.update({\"Random Forest\": RandomForestClassifier()})\n",
    "classifiers.update({\"LR\":LogisticRegression()})\n",
    "classifiers.update({\"SVC\": SVC()})\n",
    "classifiers.update({\"DTC\": DecisionTreeClassifier()})\n",
    "\n",
    "parameters = {}\n",
    "parameters.update({\"XGBOOST\": { \n",
    "                                        \"classifier__learning_rate\":[0.15,0.1,0.05,0.01,0.005,0.001], \n",
    "                                        \"classifier__n_estimators\": [200],\n",
    "                                        \"classifier__max_depth\": [2,3,4,5,6],\n",
    "                                        \"classifier__subsample\": [0.8, 0.9, 1]\n",
    "                                         }})\n",
    "\n",
    "\n",
    "parameters.update({\"Random Forest\": { \n",
    "                                    \"classifier__n_estimators\": [200],\n",
    "                                    \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                                    \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                    \"classifier__max_depth\" : [3, 4, 5, 6, 7, 8],\n",
    "                                    \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                    \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                    \"classifier__criterion\" :[\"gini\", \"entropy\"]     ,\n",
    "                                    \"classifier__n_jobs\": [-1]\n",
    "                                     }})\n",
    "\n",
    "\n",
    "parameters.update({\"LR\": {\n",
    "                          \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                          \"classifier__n_jobs\": [-1],\n",
    "                          \"classifier__C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                          \"classifier__solver\": [\"lbfgs\", \"liblinear\", \"sag\"] \n",
    "                          }})\n",
    "\n",
    "\n",
    "\n",
    "parameters.update({\"SVC\": { \n",
    "                            \"classifier__kernel\": [\"linear\"],\n",
    "                            \"classifier__gamma\": [\"auto\"],\n",
    "                            \"classifier__C\": [0.1, 0.5, 1, 5, 10, 50, 100],\n",
    "                            \"classifier__degree\": [1, 2, 3, 4, 5, 6]\n",
    "                             }})\n",
    "\n",
    "\n",
    "parameters.update({\"DTC\": { \n",
    "                            \"classifier__criterion\" :[\"gini\", \"entropy\"],\n",
    "                            \"classifier__splitter\": [\"best\", \"random\"],\n",
    "                            \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                            \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                            \"classifier__max_depth\" : [1,2,3, 4, 5, 6, 7, 8],\n",
    "                            \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                            \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                             }})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                      Tuning a classifier to use with RFECV                #\n",
    "###############################################################################\n",
    "# Define classifier to use as the base of the recursive feature elimination algorithm\n",
    "selected_classifier = \"Random Forest\"\n",
    "\n",
    "classifier = classifiers[selected_classifier]\n",
    "  \n",
    "# Define steps in pipeline\n",
    "steps = [ (\"classifier\", classifier)]\n",
    "\n",
    "\n",
    "# Initialize Pipeline object\n",
    "pipeline = Pipeline(steps = steps)\n",
    "  \n",
    "# Define parameter grid\n",
    "param_grid = parameters[selected_classifier]\n",
    "\n",
    "# Initialize GridSearch object\n",
    "gscv = GridSearchCV(pipeline, param_grid, cv = 10,  n_jobs= -1, verbose = 1, scoring = \"roc_auc\")\n",
    "                  \n",
    "# Fit gscv\n",
    "print(f\"Now tuning {selected_classifier}. Go grab a beer or something.\")\n",
    "gscv.fit(X_train, np.ravel(y_train))  \n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = gscv.best_params_\n",
    "best_score = gscv.best_score_\n",
    "        \n",
    "# Update classifier parameters\n",
    "tuned_params = {item[12:]: best_params[item] for item in best_params}\n",
    "classifier.set_params(**tuned_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Features using RFECV\n",
    "\n",
    "class PipelineRFE(Pipeline):\n",
    "    # Source: https://ramhiser.com/post/2018-03-25-feature-selection-with-scikit-learn-pipeline/\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        super(PipelineRFE, self).fit(X, y, **fit_params)\n",
    "        self.feature_importances_ = self.steps[-1][-1].feature_importances_\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#    Feature Selection: Recursive Feature Selection with Cross Validation   #\n",
    "###############################################################################\n",
    "    \n",
    "# Define pipeline for RFECV\n",
    "\n",
    "steps = [ (\"classifier\", classifier)]\n",
    "\n",
    "pipe = PipelineRFE(steps = steps)\n",
    "\n",
    "# Initialize RFECV object\n",
    "feature_selector = RFECV(pipe, cv = 10, step = 1, scoring = \"roc_auc\", verbose = 1)\n",
    "\n",
    "# Fit RFECV\n",
    "feature_selector.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Get selected features\n",
    "feature_names = X_train.columns\n",
    "selected_features = feature_names[feature_selector.support_].tolist()\n",
    "\n",
    "\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                              Performance Curve                           #\n",
    "###############################################################################\n",
    "# Get Performance Data\n",
    "performance_curve = {\"Number of Features\": list(range(1, len(feature_names) + 1)),\n",
    "                    \"AUC\": feature_selector.grid_scores_}\n",
    "performance_curve = pd.DataFrame(performance_curve)\n",
    "\n",
    "# Performance vs Number of Features\n",
    "# Set graph style\n",
    "sns.set(font_scale = 1.75)\n",
    "sns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n",
    "               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n",
    "               'ytick.color': '0.4'})\n",
    "colors = sns.color_palette(\"RdYlGn\", 20)\n",
    "line_color = colors[3]\n",
    "marker_colors = colors[-1]\n",
    "\n",
    "# Plot\n",
    "f, ax = plt.subplots(figsize=(13, 6.5))\n",
    "sns.lineplot(x = \"Number of Features\", y = \"AUC\", data = performance_curve,\n",
    "             color = line_color, lw = 4, ax = ax)\n",
    "sns.regplot(x = performance_curve[\"Number of Features\"], y = performance_curve[\"AUC\"],\n",
    "            color = marker_colors, fit_reg = False, scatter_kws = {\"s\": 200}, ax = ax)\n",
    "\n",
    "# Axes limits\n",
    "plt.xlim(0.5, len(feature_names)+0.5)\n",
    "plt.ylim(0.50, 0.90)\n",
    "\n",
    "# Generate a bolded horizontal line at y = 0\n",
    "ax.axhline(y = 0.50, color = 'black', linewidth = 1.3, alpha = .7)\n",
    "\n",
    "# Turn frame off\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Figure\n",
    "plt.savefig(\"performance_curve.png\", dpi = 1080)\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "performance_curve.sort_values('AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                 Feature Selection: Recursive Feature Selection           #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# Define pipeline for RFECV\n",
    "#steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "steps = [ (\"classifier\", classifier)]\n",
    "\n",
    "pipe = PipelineRFE(steps = steps)\n",
    "\n",
    "# Initialize RFE object\n",
    "feature_selector = RFE(pipe, n_features_to_select = 6, step = 1, verbose = 1)\n",
    "\n",
    "# Fit RFE\n",
    "feature_selector.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Get selected features labels\n",
    "feature_names = X_train.columns\n",
    "selected_features = feature_names[feature_selector.support_].tolist()\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### list classifiers with best parameters ###\n",
    "\n",
    "feature = X_train[selected_features]\n",
    "target = y_train\n",
    "\n",
    "svm = SVC(probability=True,C=0.1, degree=1, gamma='auto', kernel='linear')\n",
    "\n",
    "lr = LogisticRegression(C=0.01, n_jobs=-1, solver='sag')\n",
    "\n",
    "tree = DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
    "                       max_depth=8, max_features='sqrt', min_samples_leaf=0.05,\n",
    "                       min_samples_split=0.005)\n",
    "\n",
    "\n",
    "xgboost = xgb.XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
    "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
    "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.01, max_delta_step=None, max_depth=6,\n",
    "              min_child_weight=None, #missing=nan, \n",
    "                            monotone_constraints=None,\n",
    "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
    "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
    "              scale_pos_weight=None, subsample=0.8, tree_method=None,\n",
    "              validate_parameters=None, verbosity=None)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=8, max_features='log2', min_samples_leaf=0.005,\n",
    "                       min_samples_split=0.005, n_estimators=200, n_jobs=-1)\n",
    "\n",
    "classifier_lst = [lr, svm, tree, xgboost, rf]\n",
    "classifier_lst_name = ['LR', 'SVM', 'DT', 'XGboost', 'RF']\n",
    "\n",
    "def npv(y_true,y_pred):\n",
    "    confu = confusion_matrix(y_true,y_pred)\n",
    "    tn, fp, fn, tp = confu.ravel()\n",
    "    NPV = tn/(tn+fn)\n",
    "    return NPV\n",
    "\n",
    "def spc(y_true,y_pred):\n",
    "    confu = confusion_matrix(y_true,y_pred)\n",
    "    #print(confu,confu.sum())\n",
    "    tn, fp, fn, tp = confu.ravel()\n",
    "    TNR = tn/(tn+fp)\n",
    "    #print(TNR)\n",
    "    return TNR \n",
    "\n",
    "def prcauc(model,fea,tar):\n",
    "    X = np.array(fea)\n",
    "    y = np.array(tar)\n",
    "    aucspr = []\n",
    "    for train, test in cv.split(X, y):\n",
    "        probas_ = model.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        precision, recall, thresholds = precision_recall_curve(y[test], probas_[:, 1])\n",
    "        #precisions.append(interp(mean_recall, precision,recall))\n",
    "        pr_auc = auc(recall, precision)\n",
    "        aucspr.append(pr_auc)\n",
    "    #print(mean_precision)\n",
    "    prc_auc_CI = np.percentile(aucspr, 2.5), np.percentile(aucspr, 97.5)\n",
    "    print(\"prc_auc_CI\",prc_auc_CI)\n",
    "    return prc_auc_CI\n",
    "\n",
    "npv_score,spc_score,mcc_score = make_scorer(npv),make_scorer(spc),make_scorer(matthews_corrcoef)\n",
    "prcauc_score = make_scorer(prcauc)\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set StratifiedKFold(CV) =10 \n",
    "def make_baseline(data,classifier):\n",
    "    #feature,target = load_data(data)\n",
    "    classifier.fit(feature,target)\n",
    "    \n",
    "    acc = cross_val_score(classifier,feature,target,cv =cv)  ## accuracy \n",
    "    acc_mean = np.mean(acc) ###\n",
    "    \n",
    "    precisions = cross_val_score(classifier,feature,target,cv =cv,scoring = 'precision')   ### PPV \n",
    "    precisions_mean = np.mean(precisions)\n",
    "        \n",
    "    print(precisions)\n",
    "    \n",
    "    #PRC AUC\n",
    "    prc_aucs = prcauc(classifier,feature,target)\n",
    "    print(\"PRC AUC:\",prc_aucs)\n",
    "    \n",
    "    f1s = cross_val_score(classifier,feature,target,cv=cv,scoring='f1')\n",
    "    f1s_mean = np.mean(f1s)\n",
    "      \n",
    "    aucs = cross_val_score(classifier,feature,target,cv=cv,scoring='roc_auc') \n",
    "    auc_mean = np.mean(aucs)\n",
    "    \n",
    "    prc_auc_CI = np.percentile(prc_aucs, 2.5), np.percentile(prc_aucs, 97.5)\n",
    "    print(\"prc_auc_CI\",prc_auc_CI)\n",
    "    \n",
    "    auc_CI = np.percentile(aucs, 2.5), np.percentile(aucs, 97.5)\n",
    "    ##\n",
    "    \n",
    "    precision_CI = np.percentile(precisions, 2.5), np.percentile(precisions, 97.5)\n",
    "    f1s_CI = np.percentile(f1s, 2.5), np.percentile(f1s, 97.5)\n",
    "    acc_CI = np.percentile(acc, 2.5), np.percentile(acc, 97.5)\n",
    "    return acc_mean,precisions_mean,f1s_mean,auc_mean,auc_CI,prc_auc_CI,precision_CI,f1s_CI,acc_CI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### performance criteria\n",
    "precisions_mean_lst =[]\n",
    "f1s_mean_lst,auc_mean_lst,acc_mean_lst = [],[],[]\n",
    "precisions_CI_lst,f1s_CI_lst,acc_CI_lst = [],[],[],[]\n",
    "auc_CI_lst,prcauc_CI_list = [],[]\n",
    "for i in classifier_lst:\n",
    "    acc_mean,precisions_mean,f1s_mean,auc_mean, auc_CI,\n",
    "    prc_auc_CI,spc_CI,npv_CI,precision_CI,f1s_CI,acc_CI = make_baseline(data,i)\n",
    "    acc_mean_lst.append(acc_mean)\n",
    "    precisions_mean_lst.append(precisions_mean)\n",
    "    f1s_mean_lst.append(f1s_mean)\n",
    "    auc_mean_lst.append(auc_mean)   \n",
    "    auc_CI_lst.append(auc_CI)\n",
    "    prcauc_CI_list.append(prc_auc_CI)    \n",
    "    precisions_CI_lst.append(precision_CI)\n",
    "    f1s_CI_lst.append(f1s_CI)\n",
    "    acc_CI_lst.append(acc_CI)\n",
    "    \n",
    "print(\"acc:\",acc_mean_lst)\n",
    "print(\"precision:\",precisions_mean_lst)\n",
    "print(\"f1_score:\",f1s_mean_lst)\n",
    "print(\"auc:\",auc_mean_lst)\n",
    "print(\"auc_CI:\",auc_CI_lst)\n",
    "all_para = {\"accuracy\":acc_mean_lst,\"precision\":precisions_mean_lst,\n",
    "            \"f1score\":f1s_mean_lst,\n",
    "            \"aucscore\":auc_mean_lst,            \n",
    "            \"accuracy_CI\":acc_CI_lst,\"precision_CI\":precisions_CI_lst,\n",
    "            \"f1_score_CI\":f1s_CI_lst,\n",
    "            \"auc_CI\":auc_CI_lst,\"prc_auc_CI\":prcauc_CI_list}\n",
    "result = DataFrame(all_para,index = ['LR', 'SVM', 'DT', 'XGboost', 'RF'])\n",
    "print(result[result.columns[0:7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ROC curve\n",
    "\n",
    "X,y=np.array(feature),np.array(target)\n",
    "print(X.shape,y.shape)\n",
    "cv = StratifiedKFold(n_splits=5,random_state=1)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "from scipy import interp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def plot_ROC(model,model_name,color):\n",
    "    \n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    i = 0\n",
    "    for train, test in cv.split(X, y):\n",
    "        probas_ = model.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1]) #,pos_label=1\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        i += 1\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    roc_auc_CI = np.percentile(aucs, 2.5), np.percentile(aucs, 97.5)\n",
    "    print(\"roc_auc_CI\",roc_auc_CI)\n",
    "    plt.plot(mean_fpr, mean_tpr, color=color,\n",
    "             label=r'{} (AUC = %0.2f, 95%%CI [%0.2f, %0.2f])'.format(model_name) % (mean_auc, roc_auc_CI[0], roc_auc_CI[1]),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "color_list = ['b','g','r','c','m']\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,10)\n",
    "for model,model_name,color in zip(classifier_lst,classifier_lst_name,color_list):\n",
    "    plot_ROC(model,model_name,color)\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color=(0.6, 0.6, 0.6),label='Chance', alpha=.8)\n",
    "plt.xlim([-0.0, 1.0])\n",
    "plt.ylim([-0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "#plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\", fontsize='small')\n",
    "fig.show()\n",
    "\n",
    "plt.savefig(\"ROC.png\", dpi = 1080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
